# EM 算法

EM 算法是为了在有隐变量的条件下，也能用最大似然求模型参数  
- 观测变量X，隐变量Z，模型参数θ
- 输入为$X,Z,P(X,Z),P(Z|Y,\theta)$
- 我们想要最大化$LL=logP(X|\theta)=log\Sigma_zP(X,Z|\theta)$,但是因为隐变量z观测不到所以做不了  
- 因此，我们转而去让z先固定下来，也就是E步，求期望$Q = E_z[logP(X,Z)|Y,\theta^i]$，可以发现这个式子和上个式子的区别就在于我们这里我们给定了一个$\theta$.
- 得到的Q是一个关于$\theta$的函数，我们再对它求最大即可达到原来最大化$LL$的目的
- EM算法，可以理解为，由于不知道Z，那么我最大化LL的时候，就按自己定的一个参数$\theta$来最大化LL关于Z的期望，以求得新的$\theta$，然后再用新的$\theta$不断重复上述步骤