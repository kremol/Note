## CNN
- CNN由卷积层、池化层、全连接层三部分构成
- 卷积层相当于提取特征，池化层相当于近似表示，全连接层做一种拟合，CNN有效地减少了训练参数，降低了过拟合风险。filter相当于稀疏化了神经网络的连接
- 卷积核filter的channel数目始终与本层的输入channel数目一致



## RNN
- BRNN 是先算前向隐层，然后再算逆向隐层，最后一起算输出y
- Beam Search 实际上就是贪婪搜索的扩展版。在序列模型里，比如预测下一个词。贪婪搜索每步都找最优的词，而集束搜索可以每次保存最好的k个词，然后在下一个step的多种情况里，也是保存最好的k个情况。这样需要k个神经网络，如图


## sigmod和交叉熵损失搭配的原因
https://blog.csdn.net/u012162613/article/details/44239919  
就是用交叉熵损失，梯度中不会出现sigmod的导数，避免了梯度消失

## 梯度
https://zhuanlan.zhihu.com/p/38525412 
- 一元函数导数为在这点的变化率。二元函数在一点处可以沿多个方向有变化率，即方向导数.  
- 假设一个方向为$e_l = (cos\alpha,sin\alpha)$，那函数在一点$(x_0,y_0)$处沿该方向的方向导数值为$L = f_x(x_0,y_0)cos\alpha + f_y(x_0,y_0)sin\alpha$  
- 那哪个方向的方向导数最大呢（即函数**增大**的变化率最大），这时可以把方向导数看做两个向量的内积，$L = |(f_x,f_y)||e_l|cos\beta$    
- 所以，两个向量方向相同时，方向导数最大，因为方向导数为正值，所以函数呈上升趋势(可以类比一元函数的导数)，这个方向也就是$(f_x,f_y)$，称为梯度，最大导数值$L=|(f_x,f_y)|$  
- 所以，梯度方向函数增长最快，负梯度方向下降最快

## 神经网络初始化权重
https://segmentfault.com/a/1190000015388945  
- ReLU初始化为小的正值，效果一般比较好
- 随机初始化权重np.random.randn很可能出现问题
- 全部初始化为0一般不可取。用单隐层神经网路做例子，求导就会发现初始全取0会导致很多梯度为0，更新不动
