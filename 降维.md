# 降维
降维的目的是让模型在低维空间学习的更好
## **线性**降维方法
### 多维缩放 MDS 
要求降维后样本的距离在多维空间中得以保持  
所以整个思路为：
- 输入数据，计算距离矩阵D
- 要求D在新空间中不变，数学上可以推导出降维后的内积矩阵$B = Z^TZ$和D之间有定量关系，所以，可以由D推出B
- 有了B之后，对B做特征值分解$B = V\Lambda V^T$,所以得到$Z = \Lambda^{1/2} V^T$
- 如果要降到d维，则选$\Lambda$最大的d个特征值和对应的特征向量组成新的$\Lambda$ 和 V ,来算出Z即可


### 主成分分析PCA
如何用一个超平面来对所有样本进行恰当的表示  
基于两种考虑：
- 最近重构性：样本点到超平面的距离都足够近
- 最大可分性：样本点在这个超平面上的投影尽可能分得开

推导见图片  
结论就是，西瓜书P231 图10.5，非常简单


> 以上都假设高维到低维空间的函数映射是线性的，然而，有时候需要非线性映射才能找到恰当的低维嵌入

## 非线性降维方法
常用的方法是KPCA，先将样本映射到高维，在高维空间中进行PCA，因为PCA中是对$XX^T$这种内积形式做特征值分解，所以就可以用核技巧，直接算内积