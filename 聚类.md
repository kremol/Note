# Cluster

## 性能度量
主要有两类: 
1. 和参考模型比，称为"外部指标"&emsp; JC、FMI
2. 直接考察聚类结果，称为"内部指标"&emsp; DB、DI

## 距离
1. 闵科夫斯基距离(Minkowski distance)
$$dist = (\sum_{k=1}^n|x_{ik} - x_{jk}|^p)^{1/p}$$
- 当p->时，为切比雪夫距离
$$dist_{qbxf}(p,q) = max_i|p_i - q_i|$$
- p = 2 欧式距离
- p = 1 曼哈顿距离
$$dist_{man}(x_i,x_j) = ||x_i - x_j||_1$$

2. 无序属性的距离度量可以采用VDM:用一个属性出现的次数来衡量其值的大小

## 聚类算法
### 原型聚类
> "原型"是指样本空间中具有代表性的点  

这类算法  
- 先对原型进行初始化
- 然后对原型进行迭代更新  


一. K-Means  
1. 随机选取k个原型，每个原型代表一类
2. 计算每个x和每个原型的距离，然后把这个x归到和距离最近的原型那一类
3. 更新每个类的新原型，比如做平均
4. 当原型保持不变则收敛



二. 学习向量量化（LVQ）  
LVQ使用数据的**类别**信息来辅助聚类  
1. 输入： 
- 数据集D{(x1,y1) ... (xn,yn)},
- 原型向量个数q和各原型向量的预设类别{t1,t2....tq}**(和kmeans类似，但加了个类别属性)**
- 学习率$\eta$
2.  首先初始化{$p_1,p_2...p_q$}  
之后
- 随机选一个样本($x_j,y_j$)
- 计算$x_j$与$p_i$的距离，找出最近的$p_i$
- 查看他们的类别是否一样，如果一样则
$$p^{\prime} = p_{i*} + \eta\cdot(x_j - p_{i*})$$
否则
$$p^{\prime} = p_{i*} - \eta\cdot(x_j - p_{i*})$$
相当于类别一样则靠近，否则则远离
- 将原型向量更新为$p^{\prime}$直到满足条件停止，比如到达最大轮数

3. 聚类的时候，一个新的x就划入和它最近的原型向量那一类，注意，有多少个原型向量就有多少类，而不是看原型向量的标记  

三. 高斯混合聚类  
用混合高斯模来聚类，**假设样本由混合高斯分布生成**，我们假定有k个成分，那就要聚出来k个类  
设混合高斯为：
$$p_M(x) = \sum_{i=1}^k \alpha_i\cdotp(x|\mu_i,\Sigma_i)$$

通过EM算法，可以算出这$\alpha,\mu,\Sigma$这几个参数，然后聚类的时候，每个样本就聚到$p_M(z_j = i|x_j)$后验最大的那个类里  


### 密度聚类
此类算法假定**聚类结构能够通过样本的紧密程度确定，考察样本之间的可连接性，并基于可连接性不断扩展聚类簇以获得最终的聚类结果**

代表算法：DBSCAN  
- 通俗讲，首先定义**核心对象**的概念，指那些，在$\varepsilon$邻域里至少包含MinPts个样本的点  
- 然后如果$x_j$ 在$x_i$的邻域里，那么就说$x_j$和$x_i$是**密度直达**的，注意这两个得是核心对象。一堆核心对象，通过邻域相连接，那么他们就是**密度相连**的。  
- 所有密度相连的核心对象，包括核心对象邻域里的点，聚成一类。剩下的所有点叫做**噪声**，就OK了


### 层次聚类
试图在不同层次对数据集进行划分，从而形成树形的聚类结构  
代表算法：AGNES  
- 这是一种采用自底向上的聚类算法，首先将所有样本看成一个聚类簇
- 然后在算法的每一步找出距离最近的两个聚类簇进行合并，不断重复，直到满足要求的聚类个数。