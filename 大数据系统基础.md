# 大数据系统基础

## HDFS
大文件因为存不到一台机器上，所以需要分布式文件系统，用多个机器来存文件的不同部分。但对外看还是一个整体的文件系统
- 这样，就有一个namenode来管理整个分布式文件系统，一些datanode来管理自己的那块文件系统
- 同时需要冗余备份，一般将一个大文件的每一块都在不同的机器上存3份一样的，存的地方namenode统一管理

## MapReduce
一种处理大数据的引擎，方法
- map阶段将输入数据映射为一系列(key，value)键值对
- reduce阶段的输入为map的输出。将相同的key映射给同一个reducer处理，reducer会对某几种key做整合，最后输出最终的(key，value)键值对。


### hadoop

1. hadoop 是一个处理大数据处理框架，用了引擎是MapReduce
只需要写mapper，reducer ，hadoop就会自动对分布式文件系统中的文件执行map reduce。包括中间数据结果的存储，shuffle，sort，hdfs等等都不需要你写了，只需要你写map和reduce过程应该做什么即可。

2.编程时，mapper，reducer只要输出key \t value(用\t分开）。读入从标准输入读入。(具体见《hadoop with python》的wordcount例子，它是用的hadoop-streaming这个jar包，有些写法根据hadoop，机器不同，需要改一改)。
