# 集成学习
- 多个弱学习器组合成一个强的学习器，各个弱学习器应该“好而不同”    
- 考虑二分类问题，在基学习器误差相互独立的情况下，采用投票法，则算出来集成的错误率随着学习器数目T的增加不断下降至0,但是注意前提是误差相互独立，但这很难做到，所以基学习器的准确性和多样性存在冲突。如何产生并结合好而不同的个体学习器正是集成学习研究的核心  西瓜书P172 173   
- 弱学习器是指泛化性能略优于随机猜测的学习器  

- 根据个体学习器的生成方式，集成学习大致分为两类：
1. 个体学习器之间存在强依赖关系，必须串行生成的序列化方法，代表Boosting
2. 个体学习器间不存在强依赖关系，可同时生成的并行化方法，代表Bagging 和 随机森林

## Boosting
顾名思义，提升算法，就是将弱学习器提升为强学习器，它的工作机制为：先训练一个基学习器，然后根据基学习器的表现，调整各个训练样本的权重，使得学习错误的样本在后续受到更多关注，然后基于调整的训练样本训练下一个基学习器，如此重复，直至基学习器数目达到T，最终将这T个基学习器进行加权结合

### Adboost
每轮学习一个基学习器和它的权重，然后根据误差来改变各训练样本的权重，直到学出所有的学习器
Adboost可以看做前向学习模型以指数损失作为损失函数

### xgboost 

### GDBT
普通提升树是每次拟合残差  
而梯度提升算法，是每次拟合负梯度值（具体还不太懂）

## Bagging
Bagging很简单，是基于bootstrap sampling(自助采样)  
我们用自助采样采出T个含有m个训练样本的采样集，然后基于每个采样集训练出一个基学习器，再将这些基学习器进行结合，比如投票。这就是Bagging

## 随机森林RF
它是Bagging的一个扩展变体  
**RF在以决策树为基学习器构建Bagging集成的基础上，对决策树的训练过程引入了随机属性选择**  
- 传统决策树：在d个划分属性时选一个最优的属性  
- RF：先从属性集合中随机选择一个包含k个属性的子集，再从这个子集中选择最优一个属性进行划分，推荐k = log2d