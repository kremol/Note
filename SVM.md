# SVM

分为线性支持向量机和非线性支持向量机，非线性主要是用了核技巧

## 线性支持向量机
给一些线性可分的数据，寻找一个超平面，将它们分开  
感知机是寻找一个可以分开数据的超平面，而SVM不仅要分开，同时还要使得这个平面最优，最优就是使**所有数据点到该平面的最小距离最大**  
  
所以那个优化目标是怎么来的呢，不是一拍脑门子就是写1了。是因为根据目标我们可以把优化式子写成：
> max“所有点到平面的最小距离”（记该距离为$\gamma$）  
同时受限制于 所有点到平面的距离都大于等于$\gamma$

这样就既满足了最优，又满足了**分类正确**这一要求  
然后上下同乘以||w||就可以得到《统计学习方法7.11-7.12》  
这时候，$\gamma$可以选1的原因见图片

- 这样我们就得到了优化式子：7.13和7.14
- 接下来对这个目标的对偶形式进行求导计算得到新的目标式，
- 新的目标式是满足KKT条件，由此推出了支持向量，这点见**西瓜书p124**
- 然后用SMO算法对新目标式求得解$\alpha$
- 然后由$\alpha$求得w，之后随便选一个$\alpha^*$不为0的其对应的数据点(x,y)， 代入 y = w.x +b 就可以得到b（因为该点为支持向量）

### 软间隔
就是加了个松弛变量，同时为松弛变量加惩罚，解法还是一样的

## 核方法和非线性支持向量机
核函数是一个函数，它的计算结果巧妙的**代表**了高维空间两个变量的内积  
SVM和核函数是天作之合，是因为SVM的结果里正好就是内积。假设现在线性不可分了，那我们通过一个函数，把数据转到高维空间，在高维空间进行SVM的时候，得到的结果中就是包含点在高维空间的内积，但这很不好算。这样，我们使用核函数，就可以直接算了。

 